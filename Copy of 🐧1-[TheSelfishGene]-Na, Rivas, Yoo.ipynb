{"cells":[{"cell_type":"markdown","metadata":{"id":"HwnrpFc8x9ty"},"source":["**Kaggle team name**: üêß1-TheSelfishGene-Na, Rivas, Yoo \n","1. Format: `üêß[BB Team#]-[Your fancy Kaggle group name]-[Students' names]`. \n","  1. Eg. `üêßA-Heros-Fleischer,Melnikov`, where üêß identifies JHU and `A` identifies the Canvas group code\n","\n","Your private LB score must be reproducible with this Colab. Seed all [RNG](https://en.wikipedia.org/wiki/Random_number_generation). Don't exceed runtime quota.\n","\n","<small><hr style=\"margin:0;background-color:silver\"><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a>, ¬©<a href=\"https://apps.ep.jhu.edu/course-homepages/3765-605-742-deep-neural-networks\" target=\"_blank\">JHU</a> 2021 onwards</font></small>"]},{"cell_type":"markdown","metadata":{"id":"JbiR4YHvyX1f"},"source":["<hr color=red>\n","\n","# **üèÜüß¨Genomics**\n","\n","\n","<details><summary><font color=darkblue>More info and Kaggle API instructions</font></summary>\n","\n","[Kaggle competition URL](https://www.kaggle.com/c/3722genomics/rules). See competition rules, submission, grading, dataset, and performance metric. The **starter code** below produces a baseline model, which you should beat, while respecting the competition rules. Your code starts after the timer. This is your baseline model. Seed all [RNG](https://en.wikipedia.org/wiki/Random_number_generation) for reproducibility!\n","\n","**Instructions for enabling Kaggle API in Colab**:\n","1. Accept competition rules before running [Kaggle API](https://github.com/Kaggle/kaggle-api#api-credentials). [Loading Kaggle dataset example](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab)\n","1. In your Kaggle Account, [Create API Token](https://github.com/Kaggle/kaggle-api#api-credentials) and save the resulting **kaggle.json** file to the [root of your Google Drive](https://drive.google.com/drive/u/0/my-drive) \n","2. In Colab, open **Files** panel üóÄ (on the left) and click gray folder icon <font color=gray>üñø</font> to mount your Google drive\n","\n","Your Kaggle/Google Drive credentials are secure; and Colab's kaggle.json only lasts a Colab session.\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31689,"status":"ok","timestamp":1658717481911,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"4ZK9-jiByA4Z","outputId":"2b6d738e-cb9d-40e0-e2d9-1f8b8d7586a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if kaggle.json is stored in Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9852,"status":"ok","timestamp":1658717491759,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"dt1AeSYEx5kV","outputId":"4d933a33-9166-44f4-eb2e-e235fba52bd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat 'kaggle.json': No such file or directory\n","- competition is now set to: 3722genomics\n","100% 9.65M/9.65M [00:00<00:00, 93.0MB/s]\n","Using competition: 3722genomics\n"," teamId  teamName                                 submissionDate       score    \n","-------  ---------------------------------------  -------------------  -------  \n","8301149  JB-TheBestDNA-Katsaros,Givre,Natali      2022-03-20 18:53:39  0.98760  \n","8314244  JF-Genes-Brodsky,Hu                      2022-03-12 01:12:51  0.98710  \n","8332286  JA-GeneticCoders-Merran,Rumman           2022-03-22 00:28:12  0.98680  \n","8306609  JD-TheGeneTeam-Khan,Dinh                 2022-03-20 00:48:37  0.98670  \n","8912603  üêß2-Codebreakers-Cahill,Cui               2022-07-25 00:27:23  0.98610  \n","8908701  üêß6- Splicers -Acosta,Bollineni           2022-07-24 16:54:37  0.98400  \n","8909125  üêß4-GeneormusEmbeddings-Sahi,Samman       2022-07-24 12:55:45  0.98360  \n","8301174  JE-RippedGenes-Li, Packard               2022-03-21 00:34:10  0.98210  \n","8327811  CodeApple                                2022-03-20 03:13:06  0.98140  \n","8922200  üêß5 - AppleBottomGenes - Nathanson, Rice  2022-07-25 02:11:58  0.98060  \n","8908549  üêß1-TheSelfishGene-Na, Rivas, Yoo         2022-07-25 01:22:51  0.98040  \n","8911485  David Na                                 2022-07-25 02:02:22  0.98030  \n","8336204  JC-ATGC-Hogge,Barrett                    2022-03-20 05:05:42  0.98020  \n","8310191  JG-MeanGenes-Lee,Corson                  2022-03-20 17:26:36  0.97910  \n","8918598  üêß3- LetMeGetInYourGenes -Nilla,Nguyen    2022-07-25 01:42:37  0.97840  \n","8291816  JH-TBD-MendolaHernandez                  2022-03-21 00:00:22  0.96950  \n","8259445  üß¨ JHU baseline                           2022-03-06 00:24:53  0.94230  \n"]}],"source":["!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n","!mkdir -p ~/.kaggle                               # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n","!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n","!cp kaggle.json ~/.kaggle/kaggle.json > log       # Alternative location of kaggle.json (without a connection to Google Drive)\n","!chmod 600 ~/.kaggle/kaggle.json                  # give only the owner full read/write access to kaggle.json\n","!kaggle config set -n competition -v 3722genomics # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n","!kaggle competitions download >> log              # download competition dataset as a zip file\n","!unzip -o *.zip >> log                            # Kaggle dataset is copied as a single file and needs to be unzipped.\n","!kaggle competitions leaderboard --show           # print public leaderboard"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84095,"status":"ok","timestamp":1658717575852,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"yGUK_EcvtN8y","outputId":"d383a553-d5f8-4c44-a0d4-4fa98c216975"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 3.67 s, sys: 593 ms, total: 4.26 s\n","Wall time: 1min 24s\n"]}],"source":["%%time\n","%%capture\n","%reset -f\n","!pip -q install -U sentence-transformers > log\n","!pip -q install -U tfds-nightly tensorflow > log\n","!pip -q install -U sentence-transformers tensorflow_addons > log\n","from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\" \n","import numpy as np, pandas as pd, time, matplotlib.pyplot as plt, os, plotly, tensorflow_addons as tfa, tensorflow as tf, tensorflow.keras as keras\n","from sklearn.model_selection import train_test_split\n","from sentence_transformers import SentenceTransformer as SBERT\n","from keras.layers import Flatten, Dense\n","from sklearn.svm import SVC, LinearSVC\n","ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n","\n","class Timer():\n","  def __init__(self, lim:'RunTimeLimit'=60*5): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n","  def ShowTime(self):\n","    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n","    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n","\n","np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n","pd.set_option('max_rows', 100, 'max_columns', 100, 'max_colwidth', 100, 'precision', 2, 'display.max_rows', 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"elapsed":373,"status":"ok","timestamp":1658717576221,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"I1TC9f43ItUY","outputId":"d413845c-6814-4f7e-84e8-b5376706639c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                                        DNA\n","id                                                                                                         \n","100000  TTGATTAATAAGATTCCTTGACACCCTTTGTAAAGTTTCTATTTCGTGTGAAATATCTATCTCTTCAAATCCTTTTAATTTATCTAGGTATTTGCT...\n","100001  ATTAGTAACGGAGGATTTACTAGATGTTTGGATTTATATTCTAATTTTATTCAGGTGGAAGGGATTGTTTTATGATTCAATAGTATACAGAGAATA...\n","...                                                                                                     ...\n","119998  CGTCGGCATGCTCGGGCAGTGCGGCGGGCCAGCAGCGTGCCAGTTGTCGCGGGGCGGCCGGGCATCGCGGCGCCGGGCGGCAGCACTCCCGCGAAG...\n","119999  GCGAGGGCACGAAGGCACGACGGCAACGGCGGCGAGGAGCGCTGTGGCAACCGTCTCCGCGTTTGCGTGCGTACAGCCGAGAGCTGGTTCGCGCAG...\n","\n","[20000 rows x 1 columns]"],"text/html":["\n","  <div id=\"df-57905320-93eb-45ba-8129-a16c15fcf798\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DNA</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>100000</th>\n","      <td>TTGATTAATAAGATTCCTTGACACCCTTTGTAAAGTTTCTATTTCGTGTGAAATATCTATCTCTTCAAATCCTTTTAATTTATCTAGGTATTTGCT...</td>\n","    </tr>\n","    <tr>\n","      <th>100001</th>\n","      <td>ATTAGTAACGGAGGATTTACTAGATGTTTGGATTTATATTCTAATTTTATTCAGGTGGAAGGGATTGTTTTATGATTCAATAGTATACAGAGAATA...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>119998</th>\n","      <td>CGTCGGCATGCTCGGGCAGTGCGGCGGGCCAGCAGCGTGCCAGTTGTCGCGGGGCGGCCGGGCATCGCGGCGCCGGGCGGCAGCACTCCCGCGAAG...</td>\n","    </tr>\n","    <tr>\n","      <th>119999</th>\n","      <td>GCGAGGGCACGAAGGCACGACGGCAACGGCGGCGAGGAGCGCTGTGGCAACCGTCTCCGCGTTTGCGTGCGTACAGCCGAGAGCTGGTTCGCGCAG...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows √ó 1 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57905320-93eb-45ba-8129-a16c15fcf798')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-57905320-93eb-45ba-8129-a16c15fcf798 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-57905320-93eb-45ba-8129-a16c15fcf798');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["vX = pd.read_csv('testX/testX.csv').set_index('id')\n","tYX = pd.read_csv('trainYX/trainYX.csv').set_index('id')\n","vX"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658717576221,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"1JVbzlnuIud4","outputId":"245f4cfe-588a-4da4-cc2c-30768962952b"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚è≥ started. You have 300 sec. Good luck!\n"]}],"source":["tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"]},{"cell_type":"markdown","metadata":{"id":"NLxefmk0Iz0U"},"source":["‚ùóDo not modify the setup above."]},{"cell_type":"markdown","metadata":{"id":"3NcTKbw3KhAn"},"source":["<hr color=red>\n","\n","<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n","\n","**Student's Section** (between ‚è≥ symbols): add your code and documentation here."]},{"cell_type":"markdown","metadata":{"id":"OpyLNt3god0c"},"source":["## **Task 1. Preprocessing Pipeline**\n"," \n","Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc. \n","1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n","1. How do you evaluate the effectiveness of these elements? \n","1. What else have you tried that worked or didn't? "]},{"cell_type":"markdown","source":["1. \n"," \n","We have tried several preprocessing techniques (feature engineering and scaling) to enhance the performance (better score) and reduce the running time of the code. Details of the techniques we have used will be described step by step with corresponding codes.     \n","\n","2. \n","We tried to manipulate the data to get the best score within the given running time limit (300 sec). Due to the GPU fluctuation, we ran our code multiple times to check if our code does not run over the time limit. We have evaluated the effectiveness of pre-processing with the validation score that we had at the final stage. We accepted the method if it gave a better validation score. Otherwise, we discarded it.      \n","\n","\n","3. All the details about what we have tried and what we have accepted or not accepted are explained with the mark-down right before each code cell."],"metadata":{"id":"xf_R_5X5SDwv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtzwChJMwPYe"},"outputs":[],"source":["# from sklearn.neural_network import MLPClassifier\n","# from sklearn.linear_model import RidgeClassifier, LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, IsolationForest\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, Normalizer, PolynomialFeatures, RobustScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n","from sklearn.metrics import accuracy_score, confusion_matrix, multilabel_confusion_matrix, classification_report\n","from scipy.stats import pearsonr, spearmanr\n","from keras.layers import Flatten, Dense, Dropout\n","from keras.callbacks import EarlyStopping\n","from sklearn.decomposition import PCA\n","from imblearn.over_sampling import SMOTE, ADASYN,SVMSMOTE\n","from sklearn.feature_selection import RFE, RFECV\n","from sklearn.utils import resample\n","from numpy import quantile, where, random\n","from numpy.random import seed\n","from copy import deepcopy\n","import math\n","import tensorflow_hub as hub"]},{"cell_type":"markdown","metadata":{"id":"30xYIFXAnaPE"},"source":["**Student's answer:**"]},{"cell_type":"markdown","metadata":{"id":"PHShIZBqoQRu"},"source":["## Initialization\n","\n","In this section we utilize SBERT to transform the DNA sequences into numerical data. Code here was provided as part of the baseline model, but is needed for any EDA that we perform."]},{"cell_type":"markdown","metadata":{"id":"TTqpls_aaE_Z"},"source":["[SBERT](https://www.sbert.net) generates 384-dimensional text embedding vectors for each text entry. See [more models](https://www.sbert.net/docs/pretrained_models.html).\n","* Only reputable publicly available embedding models are allowed (SBERT, USE, MUSE, LASER, ...). We want to prevent participants' training embeddings on test data."]},{"cell_type":"markdown","source":["Our team went with the paraphrase LM due to its vast speed over other attempted models."],"metadata":{"id":"Y-9kizy82afP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfkARvtJJCum"},"outputs":[],"source":["%%capture\n","sbert = SBERT('paraphrase-MiniLM-L6-v2') # Original Model"]},{"cell_type":"markdown","source":["There were two grouping schemes our team tried when constructing our model. The first was simply splitting up the string into k groups by adding spaces between our strings. The second was using k-mers groupings of DNA. Our team found the model actually performed better embeddings with the simple grouping schema."],"metadata":{"id":"BMv2yoUn2mgR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Jjy0xWVn7H8"},"outputs":[],"source":["def group_k(seq, k=5):\n","    protein =\"\"\n","    for i in range(0, len(seq), k):\n","        group = seq[i:i + k]\n","        protein+= group + ' '\n","          \n","    return protein.strip()\n","\n","def build_kmers(sequence, ksize):\n","    kmers = []\n","    n_kmers = len(sequence) - ksize + 1\n","\n","    for i in range(n_kmers):\n","        kmer = sequence[i:i + ksize]\n","        kmers.append(kmer)\n","\n","    return kmers\n"]},{"cell_type":"markdown","source":["Our team increased the size of our training set to 0.5 opposed to just 10,000 sequences. This improved our models performance as we now had more data to construct our model with. We also take a subsample of the remaining training data and split it into 0 and 1 classes. We take an embedding of the 0 and 1 classes using the same scheme for the tEmb and vEmb. Usage of these elements are discussed in the Preprocessing portion of step 2."],"metadata":{"id":"3Pwa6mzp3EU1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132941,"status":"ok","timestamp":1658717739189,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"_586zfPPJD4C","outputId":"5fab8d6c-8652-4b60-e0e3-87e082e4f087"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 2min 11s, sys: 1.49 s, total: 2min 13s\n","Wall time: 2min 12s\n"]}],"source":["%%time\n","tYX, tYX_unused = train_test_split(tYX, train_size=0.5, random_state=22, shuffle = True, stratify = tYX.Y)\n","tEmb, vEmb = sbert.encode([group_k(s, k=3) for s in tYX.DNA],batch_size =1000, normalize_embeddings=False), sbert.encode([group_k(s, k=3) for s in vX.DNA],batch_size =1000,normalize_embeddings=False)\n","\n","## SEE DOCUMENTATION IN TASK TWO FOR USAGE INFORMATION\n","tYX_unused_class0 = tYX_unused[tYX_unused.Y==0].head(10000)\n","tYX_unused_class1 = tYX_unused[tYX_unused.Y==1].head(10000)\n","embeddings_train_feature1 = sbert.encode([group_k(s, k=3) for s in tYX_unused_class0.DNA])\n","embeddings_train_feature0 = sbert.encode([group_k(s, k=3) for s in tYX_unused_class1.DNA])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14dxEgdLm96A"},"outputs":[],"source":["# print(f'Train embedding matrix size:', tEmb.shape)\n","# pd.DataFrame(vEmb[:3,:30], index=[x[:20]+'...' for x in vX.DNA[:3]]).style.background_gradient(cmap='coolwarm', axis=1).set_precision(2)  # show movie description and a few of its embedding features"]},{"cell_type":"markdown","metadata":{"id":"x_gwlNxuym5v"},"source":["## Exploratory Data Analysis\n","\n","Analyzing our target variable and features."]},{"cell_type":"markdown","metadata":{"id":"O9BSQW5AylEy"},"source":["### Target Variable\n","\n","Looking at the distribution of our target variable, we can see Y is a binary categorical variable. It also seems the distribution of our Y variable is fairly uniform. We most likely do not need to perform SMOTE or any kind of undersampling. However, what we do notice below is that our Training set is half the size of our testing set. Looking at the initial length of tX, we saw that it was only 10,000 in length compared to 20,000 for the test set. tYX in total is 100,000 datapoints, so we instead take 50,000 datapoints taken randomly from the Y variables. You can see this new count below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqAIRlaxorHn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658717739190,"user_tz":240,"elapsed":10,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"}},"outputId":"e971f960-814b-4387-f4af-59575ff039db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    25032\n","1    24968\n","Name: Y, dtype: int64"]},"metadata":{},"execution_count":11}],"source":["tYX.Y.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"saAngaYIyknd","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1658717739190,"user_tz":240,"elapsed":8,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"}},"outputId":"5d9f3b8f-97ab-4f85-b992-345cf8892d88"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f326d3b1190>"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["tYX.Y.hist(figsize=(10,5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"recDeOufpBOz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658717739191,"user_tz":240,"elapsed":6,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"}},"outputId":"1e3f65c3-eb42-4d69-d827-10ca2de1e02c"},"outputs":[{"output_type":"stream","name":"stdout","text":["50000\n","20000\n"]}],"source":["print(len(tYX)); print(len(vX))"]},{"cell_type":"markdown","metadata":{"id":"mcv1zow5oITK"},"source":["### Features\n","\n","In this section we analyze our features and their underlying distributions."]},{"cell_type":"markdown","metadata":{"id":"ycgjPJISulyV"},"source":["At first glance looking at the describe of our features dataframe, we can see that the means for certain columns are rougly equal to their median, suggesting a somewhat symmetric distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfbXNM_1tyNS"},"outputs":[],"source":["pd.set_option('display.max_rows', 10)\n","tEmb_eda = pd.DataFrame(tEmb.copy(), index = tYX.index)\n","# tEmb_eda.describe()"]},{"cell_type":"markdown","metadata":{"id":"GwQtkvqmu6nd"},"source":["Looking at the specific features in the data that have a mean that is outside of 10% of the median, we can see the distributions below. Looking at the histograms below, even for the features that are in this group, the distributions look fairly normal. It seems the algorithm SBERT generates these features using a fairly normal distribution, with slight deviations in mean which displace the distribution slightly left or right of zero. In general, there does not seem to be any inconsistentcy in the distributions of features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DU9jaUwaujrf"},"outputs":[],"source":["# means, medians = tEmb_eda.mean(), tEmb_eda.median()\n","# out_of_mean_range = list()\n","# for x in range(len(means)):\n","#   if (abs(means[x]) < abs(medians[x]) *0.9 or abs(means[x]) > abs(medians[x]) * 1.1) and abs(means[x]) > 0.05:\n","#     out_of_mean_range.append(x)\n","# _=tEmb_eda[out_of_mean_range].hist(figsize=(20,20))"]},{"cell_type":"markdown","metadata":{"id":"GgCFHO2pp1ff"},"source":["### Reducing Dimensionality\n","\n","In this section we try to reduce the dimensionality of our data. We implement one method using PCA. Another method is using RFE."]},{"cell_type":"markdown","metadata":{"id":"Jp9f3UXFtWOz"},"source":["#### Principal Component Analysis - UNUSED\n","\n","Although PCA did help to reduce our feature set, due to the fact we concluded our model was linearly separable. However, keeping all the features improved our models performance overall, so we chose to exclude this from our model.\n","\n","With 250+ features, the natural first idea was to perform some sort of feature reduction. Looking at the graph generated below using PCA, we were able to find that with about 95 components we are able to caputre 95% of the variance. Thus, we decided to try this in our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYG_hHk1oHsQ"},"outputs":[],"source":["# Vectors from SBERT are normalized already'\n","# # pipeline= Pipeline(steps=[\n","# #     (\"std\", StandardScaler())\n","# #   ])\n","# tEmb_eda = pd.DataFrame(tEmb.copy(), index = tYX.index)\n","\n","# # pca = PCA().fit(pipeline.fit_transform(tEmb_eda))\n","# pca = PCA().fit(tEmb_eda)\n","\n","\n","# plt.rcParams[\"figure.figsize\"] = (20,6)\n","\n","# fig, ax = plt.subplots()\n","# xi = np.arange(1, len(tEmb_eda.columns)+1, step=1)\n","# y = np.cumsum(pca.explained_variance_ratio_)\n","\n","# _=plt.ylim(0.0,1.1)\n","# _=plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","# _=plt.xlabel('Number of Components')\n","# _=plt.xticks(np.arange(1, len(tEmb_eda.columns)+1, step=10))\n","# _=plt.ylabel('Cumulative variance (%)')\n","# _=plt.title('The number of components needed to explain variance')\n","\n","# _=plt.axhline(y=0.95, color='r', linestyle='-')\n","# _=plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","\n","# ax.grid(axis='x')\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XXMX4PtcpzZe"},"source":["#### Recursive Feature Elimination - UNUSED\n","\n","RFE was able to reduce our feature set to about 195 features. However, we found a more optimal way to increase the effectiveness of our features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlfUsu7aqCEl"},"outputs":[],"source":["# rf = RandomForestClassifier(n_estimators = 50, max_depth=8, min_samples_split = 3, min_samples_leaf = 2, random_state=22)\n","# rfe = RFE(estimator=rf, verbose = 3, step=5)\n","# tEmb_train, tEmb_test, y_train, y_test = train_test_split(tEmb, tYX.Y, train_size = 0.6, random_state = 22, stratify = tYX.Y)\n","# rfe.fit(tEmb_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BOT4goBvBBi"},"outputs":[],"source":["# features_selected = [3, 4, 6, 8, 9, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 28, 31, 33, 34, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 54, 55, 57, 64, 66, 67, 69, 70, 71, 72, 73, 77, 79, 80, 81, 82, 85, 86, 89, 90, 94, 95, 97, 99, 101, 102, 104, 107, 109, 111, 113, 114, 115, 116, 117, 119, 122, 126, 127, 129, 130, 131, 134, 135, 137, 139, 140, 143, 144, 145, 147, 149, 150, 153, 157, 161, 162, 163, 172, 178, 180, 181, 185, 186, 188, 189, 190, 192, 196, 198, 200, 202, 204, 206, 207, 209, 212, 213, 214, 215, 216, 217, 218, 220, 221, 224, 226, 227, 229, 230, 231, 233, 234, 238, 240, 245, 246, 247, 249, 253, 254, 255, 256, 259, 261, 264, 265, 267, 268, 270, 271, 272, 276, 282, 286, 287, 289, 298, 299, 300, 302, 307, 308, 309, 310, 311, 312, 316, 322, 323, 325, 328, 329, 336, 338, 339, 340, 342, 343, 350, 351, 354, 355, 357, 358, 359, 360, 361, 364, 365, 366, 370, 371, 372, 373, 377, 379, 380, 382, 383]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pt5q7bpXvEoR"},"outputs":[],"source":["# tEmb_rfe = tEmb_eda.iloc[:,features_selected]"]},{"cell_type":"markdown","metadata":{"id":"z16Zc3Gy3PVG"},"source":["PCA With RFE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3NVuVnrsCaG"},"outputs":[],"source":["# pipeline= Pipeline(steps=[\n","#     (\"std\", StandardScaler())\n","#   ])\n","\n","# pca = PCA().fit(pipeline.fit_transform(tEmb_rfe))\n","\n","# plt.rcParams[\"figure.figsize\"] = (20,6)\n","\n","# fig, ax = plt.subplots()\n","# xi = np.arange(1, len(tEmb_rfe.columns)+1, step=1)\n","# y = np.cumsum(pca.explained_variance_ratio_)\n","\n","# _=plt.ylim(0.0,1.1)\n","# _=plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","# _=plt.xlabel('Number of Components')\n","# _=plt.xticks(np.arange(1, len(tEmb_rfe.columns)+1, step=10))\n","# _=plt.ylabel('Cumulative variance (%)')\n","# _=plt.title('The number of components needed to explain variance')\n","\n","# _=plt.axhline(y=0.95, color='r', linestyle='-')\n","# _=plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","\n","# ax.grid(axis='x')\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jFgC14d4Rzoa"},"source":["### Preprocessing Pipeline\n","\n","Using the number of components we computed from the analysis above, we are able to now create a preprocessing pipeline that will return a our transformed data. We remove the standard scaler because SBERT returns normalized vectors for us already."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tosnrLhPDIX"},"outputs":[],"source":["def preprocess_pipe(X=[], n_comp=[], use_pca=True):\n","    pipeline = Pipeline(steps=[('scaler', StandardScaler())])\n","\n","    for i in range(len(X)):\n","      X[i] = pipeline.fit_transform(X[i])\n","      if use_pca:\n","        X[i] = PCA(n_components=n_comp[i]).fit_transform(X[i])\n","    return np.concatenate(X, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"4n_lL-bSxLou"},"source":["### Feature Engineering\n","\n","In this section we implement a function that will build new features within our data. We take the counts and DNA segment length as two featurse. Using the counts we compute the pairwise nucleotide content within our data. After performing EDA, we found gc and ta content were significant to helping our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXQDakXsxR_P"},"outputs":[],"source":["def build_features(df_tYX):\n","    df_tYX['cntC'] = [s.count('C') for s in df_tYX.DNA]\n","    df_tYX['cntT'] = [s.count('T') for s in df_tYX.DNA]\n","    df_tYX['cntG'] = [s.count('G') for s in df_tYX.DNA]\n","    df_tYX['cntA'] = [s.count('A') for s in df_tYX.DNA]\n","    ctContent = ((df_tYX.cntC+df_tYX.cntT)+1)/((df_tYX.cntC+df_tYX.cntG+df_tYX.cntA+df_tYX.cntT)+1)\n","    df_tYX['ctContent'] = ctContent\n","    gcContent = ((df_tYX.cntC+df_tYX.cntG)+1)/((df_tYX.cntC+df_tYX.cntG+df_tYX.cntA+df_tYX.cntT)+1)\n","    df_tYX['gcContent'] = gcContent\n","    caContent = ((df_tYX.cntC+df_tYX.cntA)+1)/((df_tYX.cntC+df_tYX.cntG+df_tYX.cntA+df_tYX.cntT)+1)\n","    df_tYX['caContent'] = caContent\n","    tgContent = ((df_tYX.cntT+df_tYX.cntG)+1)/((df_tYX.cntC+df_tYX.cntG+df_tYX.cntA+df_tYX.cntT)+1)\n","    df_tYX['tgContent'] = tgContent\n","    taContent = ((df_tYX.cntT+df_tYX.cntA)+1)/((df_tYX.cntC+df_tYX.cntG+df_tYX.cntA+df_tYX.cntT)+1)\n","    df_tYX['taContent'] = taContent\n","    gaContent = ((df_tYX.cntG+df_tYX.cntA)+1)/((df_tYX.cntC+df_tYX.cntG+df_tYX.cntA+df_tYX.cntT)+1)\n","    df_tYX['gaContent'] = gaContent\n","    df_tYX['dnaSegLen'] = [len(i) for i in df_tYX.DNA]\n","    return df_tYX\n","\n","new_features = ['cntC', 'cntT', 'cntG', 'cntA', 'gcContent', 'ctContent', \n","                'caContent', 'tgContent', 'taContent', 'gaContent', 'dnaSegLen']\n"]},{"cell_type":"markdown","metadata":{"id":"KGJRwzqHob4o"},"source":["## **Task 2. Modeling Approach**\n","Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful. \n","\n","1. How did these decisions guide you in modeling?\n","1. How do you evaluate the effectiveness of these elements? \n","1. What else have you tried that worked or didn't? "]},{"cell_type":"markdown","metadata":{"id":"zi6ZjgtWnb58"},"source":["**Student's answer:**"]},{"cell_type":"markdown","source":["1. \n"," \n","We have implemented two different models (DNN and SVC). Based on our experiment, SVC gave better validation score even though DNN sometimes give better training score. So, we have decided that DNN has more risk on overfitting than the linear SVC. So we have decided to maximize our score based on SVC rather than DNN because SVC gave better score from the Kaggle leader board. We have concluded that linear SVC would be better model than DNN for generalization for this data set.\n","\n","2. \n","* <b> Scoring metric:</b>\n","We used the classification accuracy as our scoring metric because that is the requirement for this competition. We have calculated the accuracy by comparing the class label between the actual one and the predcited one from the traning set.  \n","\n","* <b> Running time:</b>\n","There is a running time limit for this competition (300 sec) . We measured the execution time to check if the code can be run within the time limit. To expedite our calcuation, we used GPU and checked the running time by multiple trials to see if the entire code can run within the time limit.\n","\n","* <b> Overfitting:</b>\n","To evaludate the overfitting, we compared the training score and validation score. Also, we checked the score from the Kaggle leader board to see if our model overfits. \n","\n","3. Details about what we have tried are specified with the markdown on top of its own code cell. \n"],"metadata":{"id":"ApSNcBo2VJrG"}},{"cell_type":"markdown","metadata":{"id":"Rq_PExUnOruZ"},"source":["#### DNN Classifier\n","\n","Our first attempt to build the model was with a DNN and DNN bagging classifier. The code implementation can be seen below. We utilized custom lr decays and bagging; however, we struggled to find a model that generalized well as we ran into the constant problem of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0ZARvtEDYUM"},"outputs":[],"source":["class MonteCarloDropout(keras.layers.Dropout):\n","    def call(self, inputs):\n","        return super().call(inputs, training=True)\n","\n","\n","def build_model(n_hidden=5, n_neurons=300, learning_rate=0.5, input_shape=[250], \n","                metrics=['accuracy'], \n","                dropout_rate = 0.2, \n","                kernel_reg = 0.01,\n","                opt_type='SGD'):\n","    Init = tf.keras.initializers.HeUniform(seed=22) \n","    InitOutput = tf.keras.initializers.GlorotNormal(seed=22) \n","    kernel_reg = keras.regularizers.L2(kernel_reg)\n","    model = keras.models.Sequential()\n","    model.add(keras.layers.Flatten(input_shape=input_shape, name='input'))\n","    # model.add(keras.layers.Dense(n_neurons, input_shape=input_shape, activation=\"elu\", kernel_initializer=Init, kernel_regularizer=kernel_reg, name=f\"input_layer\"))\n","    model.add(keras.layers.BatchNormalization())\n","    for layer in range(n_hidden):\n","        model.add(keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=Init, kernel_regularizer=kernel_reg, name=f\"hidden_{layer+1}\"))\n","        # model.add(tf.keras.layers.SimpleRNN(n_neurons, activation=\"elu\", kernel_initializer=Init, name=f\"hidden_{layer+1}\"))\n","        # model.add(keras.layers.Dropout(rate=dropout_rate, seed = 22))\n","        model.add(keras.layers.BatchNormalization())\n","    model.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer=InitOutput, name=\"output\"))\n","    if opt_type == 'SGD':\n","        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n","    elif opt_type == 'Adamax':\n","        optimizer = keras.optimizers.Adamax(learning_rate=learning_rate)\n","    elif opt_type == 'Adagrad':\n","      optimizer = keras.optimizers.Adagrad(learning_rate=learning_rate)\n","    elif opt_type == 'Nadam':\n","        optimizer = keras.optimizers.Nadam(learning_rate=learning_rate)\n","    elif opt_type == 'Ftrl':\n","        optimizer = keras.optimizers.Ftrl(learning_rate=learning_rate)\n","    else:\n","        raise exception(\"Bad Optimizer, only SGD, Adamax, Nadam and Ftrl\")\n","    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=metrics)\n","    # model.summary()\n","    return model\n","\n","\n","def lr_scheduler_decay(epoch, lr):\n","    # if epoch % 5 == 0 and epoch != 0:\n","    #   decay = 0.5\n","    # else:\n","    #   decay = 1\n","    decay = 0.97\n","    return lr * decay\n","\n","def lr_scheduler_decay_batch(epoch, lr):\n","    if epoch % 10 == 0 and epoch != 0:\n","        decay = 0.5\n","    else:\n","        decay = 1\n","    # decay = 0.97\n","    return lr * decay\n","\n","class BaggingKerasClassifier:\n","    members = []\n","\n","    def __init__(self, estimator, n_estimators=10):\n","        self.estimator = estimator\n","        self.n_estimators = n_estimators\n","        self.members = list()\n","\n","    def fit(self, X, y):\n","        member = deepcopy(self.estimator)\n","        for i in range(self.n_estimators):\n","            X_rs, y_rs = resample(X, y, random_state = 22, stratify = y)\n","            X_train, X_test, y_train, y_test = train_test_split(X_rs, y_rs, test_size=0.2, shuffle = True, random_state=22, stratify = y_rs)\n","            X_test = member['scaler'].fit_transform(X_test)\n","            _=member.set_params(model__validation_data=(X_test, y_test))\n","            _=member.fit(X_train,y_train)\n","            self.members.append(member)\n","        return self.members\n","\n","    def predict(self, X):\n","        # make predictions\n","        yhats = [model.predict(X) for model in self.members]\n","        yhats = np.array(yhats)\n","        # sum across ensemble members and divide by number of estimators\n","        summed = np.sum(yhats, axis=0)/self.n_estimators\n","\n","        # return the rounded result\n","        result = (summed>0.5)*1\n","        return result\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KIeqvZ0ceFo"},"outputs":[],"source":["# keras.backend.clear_session()\n","# with tf.device('/device:GPU:0'):\n","#     tY = tYX.Y.copy()\n","#     tX = preprocess_pipe([tEmb], use_pca=False)\n","\n","    \n","#     tf.random.set_seed(22)   # always seed your experiments\n","#     fm_test = build_model(n_hidden=5, n_neurons=500, learning_rate=0.05, input_shape=[tX.shape[1]], \n","#                 metrics=['accuracy'], \n","#                 # dropout_rate = 0.2,\n","#                 kernel_reg = 0.1,\n","#                 opt_type='SGD')\n","    \n","#     lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler_decay)\n","#     lr_decay_callback_batch = tf.keras.callbacks.LearningRateScheduler(lr_scheduler_decay_batch)\n","\n","# #     fm_test.summary()\n","    \n","#     skf = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=22)\n","#     tX_reset = tX\n","#     tY_reset = tY.reset_index(drop=True)\n","#     set_num = 1\n","#     tf.random.set_seed(22)   # always seed your experiments\n","#     for train_index, test_index in skf.split(tX_reset, tY_reset):\n","#         X_train, X_test = tX_reset[train_index],tX_reset[test_index]\n","#         y_train, y_test = tY_reset.loc[train_index], tY_reset.loc[test_index]\n","#         # keras.backend.set_value(fm_test.optimizer.learning_rate, 0.05)\n","#         # hist2 = fm_test.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=1200)\n","#         keras.backend.set_value(fm_test.optimizer.learning_rate, 0.05)\n","#         hist1 = fm_test.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=1200, callbacks=[lr_decay_callback])\n","#         keras.backend.set_value(fm_test.optimizer.learning_rate, 0.0005)\n","#         hist2 = fm_test.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test), batch_size=1200)\n","#         set_num += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaXUaDBonxzm"},"outputs":[],"source":["# for x in range(0,25):\n","#   threshold = 0.4 + x * 0.01\n","#   print(threshold)\n","#   predicted_tX = ((pd.DataFrame(fm_test.predict(tX), columns=['y'])>0.5)*1)\n","#   print(classification_report(predicted_tX.astype(int), tYX.Y, digits=6))\n","#   print(\"----------------------------------------------------------\")\n","#   test_acc = accuracy_score(predicted_tX.astype(int), tYX.Y)\n","#   print(\"Test Score: \", test_acc)\n","#   print(\"----------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQ_2OxnfnyEx"},"outputs":[],"source":["# # vEmb_proccessed = preprocess_pipe(vEmb, n_comp = 95)\n","# vEmb_proccessed = preprocess_pipe([vEmb], use_pca=False)\n","# pY = ((pd.DataFrame(fm_test.predict(vEmb_proccessed), index=vX.index, columns=['y'])>0.5)*1)   # predicted targets\n","# ToCSV(pY, 'DNN_Regularized_SBERT_3Words')"]},{"cell_type":"markdown","metadata":{"id":"Rs59kn2e4qU2"},"source":["#### Bagging Neural Network Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXvX-s3UDtIC"},"outputs":[],"source":["# keras.backend.clear_session()\n","# with tf.device('/device:GPU:0'):\n","#   tX_ohe = tEmb.copy()\n","#   tY_ohe = tYX.Y.copy()\n","\n","#   tf.random.set_seed(22)   # always seed your experiments\n","#   col = len(tX_ohe[0])\n","#   ct = ColumnTransformer([\n","#             ('scaler', StandardScaler(), list(range(0, col)))\n","#         ], remainder='passthrough')\n","\n","\n","#   keras.backend.clear_session()\n","#   lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler_decay)\n","#   lr_decay_callback_batch = tf.keras.callbacks.LearningRateScheduler(lr_scheduler_decay_batch)\n","#   keras_classifier = keras.wrappers.scikit_learn.KerasClassifier(build_model,\n","#                                                                  n_hidden=6, \n","#                                                                  n_neurons=300, \n","#                                                                  learning_rate=0.1, \n","#                                                                  input_shape=[tX_ohe.shape[1]], \n","#                                                                  epochs=60, \n","#                                                                  batch_size=800, \n","#                                                                  callbacks = [lr_decay_callback], \n","#                                                                  validation_data = None,\n","#                                                                  validation_split = 0.2,\n","#                                                                  verbose=2)\n","  \n","#   ## Start with a prefit model that is fit on the original data before bagging\n","#   tX_ohe_test = ct.fit_transform(tX_ohe)\n","#   keras_classifier.fit(tX_ohe_test, tY_ohe)\n","\n","#   pipeline = Pipeline([('scaler', ct), ('model', keras_classifier)])\n","\n","#   _=pipeline.set_params(model__validation_split = None, \n","#                         model__epochs = 60, \n","#                         model__learning_rate = 0.005,\n","#                         model__callbacks = [lr_decay_callback_batch],\n","#                         model__verbose = 2\n","#                         )\n","\n","#   bg_classifier = BaggingKerasClassifier(estimator=pipeline, n_estimators=5)\n","\n","#   models = bg_classifier.fit(tX_ohe, tY_ohe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tH-v0s4UyVDA"},"outputs":[],"source":["# predicted_tX = ((pd.DataFrame(bg_classifier.predict(tX_ohe), columns=['y'])>0.5)*1)\n","# print(classification_report(predicted_tX.astype(int), tYX.Y, digits=6))\n","# print(\"----------------------------------------------------------\")\n","# test_acc = accuracy_score(predicted_tX.astype(int), tYX.Y)\n","# print(\"Test Score: \", test_acc)\n","# print(\"----------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TJXXdry54LD"},"outputs":[],"source":["# pY_bg = ((pd.DataFrame(bg_classifier.predict(vEmb), index=vX.index, columns=['y'])>0.5)*1)   # predicted targets\n","# ToCSV(pY_bg, 'DeepNeuralNetwork_BaggingClassifier_ADAM')\n","# pY_bg.hist()"]},{"cell_type":"markdown","metadata":{"id":"39ZuuXa367EU"},"source":["### Linear SVC + Bagging\n","\n","We attempted to try SVC with bagging to help generalize our model. However, performance did not improve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNlCBh3l6u5e"},"outputs":[],"source":["# tX = tEmb_rfe.copy()\n","# tX = preprocess_pipe(tX, n_comp=65)\n","# pf = PolynomialFeatures(degree=2)\n","# tX = pf.fit_transform(tX)\n","\n","# m_baseline = LinearSVC(random_state=0, max_iter=10000, loss = 'squared_hinge', C=1, dual=True, penalty = 'l2', fit_intercept = False, intercept_scaling=1)  # SVC is ok to use. See updated Rules.\n","# m_baseline.fit(tX, tYX.Y)\n","# # %time m_baseling_bg = BaggingClassifier(m_baseline, n_estimators = 5, max_features=1.0, random_state=22).fit(tEmb, tYX.Y)\n","# m_baseline.score(tX, tYX.Y)   # in-sample accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6OaT2Z-7EK4"},"outputs":[],"source":["# pY_baseline_bag = (pd.DataFrame(m_baseline.predict(vEmb), index=vX.index, columns=['y'])>0.5)*1  # predicted targets\n","# ToCSV((pY_baseline>0.5)*1, 'Baseline_updated')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Yu-8XMo-kF5"},"outputs":[],"source":["# predicted_tX = ((pd.DataFrame(m_baseling_bg.predict(tX_ohe), columns=['y'])>0.5)*1)\n","# print(classification_report(predicted_tX.astype(int), tYX.Y, digits=6))\n","# print(\"----------------------------------------------------------\")\n","# test_acc = accuracy_score(predicted_tX.astype(int), tYX.Y)\n","# print(\"Test Score: \", test_acc)\n","# print(\"----------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"EQPnZI7fvNWr"},"source":["### Linear SVC"]},{"cell_type":"markdown","metadata":{"id":"DE7j3yekVYli"},"source":["#### Polynomial Features - UNUSED\n","\n","Using polynomial features actually decreased our score due to the trade off between lowering the number of components vs adding polynomial interactions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7S-L3EFJHqB"},"outputs":[],"source":["# with tf.device('/device:GPU:0'):\n","#   tEmb_pca = preprocess_pipe([tEmb], n_comp=[80])\n","#   pf = PolynomialFeatures(degree=2)\n","#   tEmb_pca = pf.fit_transform(tEmb_pca)\n","\n","#   %time m_baseline = LinearSVC(random_state=0, max_iter=3000, C=0.5).fit(tEmb_pca, tYX.Y)  # SVC is ok to use. See updated Rules.\n","#   m_baseline.score(tEmb_pca, tYX.Y)   # in-sample accuracy\n","\n","#   # vEmb_pca = preprocess_pipe([vEmb], n_comp=[50])\n","#   # pf = PolynomialFeatures(degree=2)\n","#   # vEmb_pca = pf.fit_transform(vEmb_pca)\n","#   # pY_baseline_poly = pd.DataFrame(m_baseline.predict(vEmb_pca), index=vX.index, columns=['y'])   # predicted targets\n","#   # ToCSV((pY_baseline>0.5)*1, 'LinearSVC_4partSBERT_all_MiniLM_L6')\n"]},{"cell_type":"markdown","metadata":{"id":"bIZ-fCZXays2"},"source":["#### K-means Clustering\n","\n","Using k-means clustering to add the label as a feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SaFHVZ9abo-m"},"outputs":[],"source":["from sklearn.cluster import MiniBatchKMeans\n","\n","def apply_k_means(df,k):\n","  scaler=StandardScaler()\n","  df_scaled=pd.DataFrame(scaler.fit_transform(df))\n","  minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=0)\n","  minibatch_kmeans.fit(df_scaled)\n","  clusters = pd.DataFrame(minibatch_kmeans.predict(df_scaled), columns=[\"k_means_cluster\"])\n","  clusters_dummy=pd.get_dummies(clusters[\"k_means_cluster\"],prefix=\"cluster\")\n","  result=pd.concat([df_scaled,clusters_dummy],axis=1)\n","  return result\n","  "]},{"cell_type":"markdown","metadata":{"id":"0_qEqmRhMP7l"},"source":["#### Preprocessing\n","\n","Our data preprocessing pipeline including the KMeans implementation and cosine similarity implementation"]},{"cell_type":"markdown","source":["#### Cosine Similarity + KMeans\n","\n","One more feature that we decided to add to our model was a cosine similarity score for 0 or 1 classes. To implement this cosine similarity, first we make use of the split data that we initially made embeddings on. In our first model creation step, we split our data by 0.5. We take a subsample of the remaining data (10000 observations), denominated by their underlying class (0 or 1). Finally, we use the util.cos similarity score and take the mean of the scores of each training and test observation relative to subsample embedding we created. Each observation in the test and train sample gets its own average cos similarity score added as a feature to the model for both classes 0 and 1:\n","\n","\n","\n","*   0Score = mean of cos_sim(50000 test obs, 10000 holdout class 0 obs) per row and repeated for vEmb\n","*   1Score = mean of cos_sim(50000 test obs, 10000 holdout class 1 obs) per row and repeated for vEmb\n","\n"],"metadata":{"id":"7nS4g44MPs4k"}},{"cell_type":"code","source":["from sentence_transformers import util\n","\n","zero_scores = util.cos_sim(tEmb,embeddings_train_feature0)\n","one_scores = util.cos_sim(tEmb,embeddings_train_feature1)\n","zero_scores_test = util.cos_sim(vEmb,embeddings_train_feature0)\n","one_scores_test = util.cos_sim(vEmb,embeddings_train_feature1)"],"metadata":{"id":"iHqbOBA4PvQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tYX = build_features(tYX)\n","tEmb_train = pd.DataFrame(tEmb, index=tYX.index, columns=pd.Series(range(0,384), dtype=\"string\").tolist())\n","tEmb_train['0score'] = 0\n","tEmb_train['1score'] = 1\n","\n","for row in range(len(tEmb_train)):\n","    tEmb_train.iloc[row,384] = zero_scores[row].mean().item()\n","    tEmb_train.iloc[row,385] = one_scores[row].mean().item()\n","\n","tEmb_train = pd.concat([tEmb_train, tYX.loc[:,new_features]], axis=1)\n","tEmb_train = apply_k_means(tEmb_train,6)\n","tEmb_train = preprocess_pipe([tEmb_train], use_pca=False)"],"metadata":{"id":"MZ_feGw8Pw4R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658717805138,"user_tz":240,"elapsed":53862,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"}},"outputId":"ada7067b-04c9-441b-9524-c04ee604a19e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["vX = build_features(vX)\n","vEmb_test = pd.DataFrame(vEmb, index=vX.index, columns=pd.Series(range(0,384), dtype=\"string\").tolist())\n","vEmb_test['0score'] = 0\n","vEmb_test['1score'] = 1\n","for row in range(len(vEmb_test)):\n","    vEmb_test.iloc[row,384] = zero_scores_test[row].mean().item()\n","    vEmb_test.iloc[row,385] = one_scores_test[row].mean().item()\n","\n","vEmb_test = pd.concat([vEmb_test, vX.loc[:,new_features]], axis=1)\n","vEmb_test = apply_k_means(vEmb_test,6)\n","vEmb_test = preprocess_pipe([vEmb_test], use_pca=False)"],"metadata":{"id":"hX-WztSMPypM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658717816936,"user_tz":240,"elapsed":11800,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"}},"outputId":"8ecc171c-59f9-42a8-e7db-1cec0b6ddf58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n"]}]},{"cell_type":"markdown","source":["#### UNUSED OLD SECTION FOR PREPROCESSING"],"metadata":{"id":"xlhRV26NVLM2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMo7QUneQe2T"},"outputs":[],"source":["# tYX = build_features(tYX)\n","# tEmb_train = pd.DataFrame(tEmb, index=tYX.index, columns=pd.Series(range(0,384), dtype=\"string\").tolist())\n","# tEmb_train = pd.concat([tEmb_train, tYX.loc[:,new_features]], axis=1)\n","# tEmb_train = apply_k_means(tEmb_train,6)\n","# tEmb_train = preprocess_pipe([tEmb_train], use_pca=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ci1Dn_wIuVFB"},"outputs":[],"source":["# vX = build_features(vX)\n","# vEmb_test = pd.DataFrame(vEmb, index=vX.index, columns=pd.Series(range(0,384), dtype=\"string\").tolist())\n","# vEmb_test = pd.concat([vEmb_test, vX.loc[:,new_features]], axis=1)\n","# vEmb_test = apply_k_means(vEmb_test,6)\n","# vEmb_test = preprocess_pipe([vEmb_test], use_pca=False)"]},{"cell_type":"markdown","metadata":{"id":"zpLGIu2iimVl"},"source":["#### Final Model\n","\n","Our final model utilizes LinearSVC without bagging as we found bagging did not help generalize our model. We split our data by 0.8 to test our model on a validation set. We tuned our C regularization to the best, most appropriate level to achieve the highest validation score."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5825,"status":"ok","timestamp":1658717838637,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"OfGARf49F_ed","outputId":"704741e4-1aa2-4791-d0e4-856845541a9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Accuracy:  0.980975\n","Validation Accuracy:  0.9806\n","Accuracy:  0.9809\n"]}],"source":["with tf.device('/device:GPU:0'):\n","  \n","\n","  X_train, X_test, y_train, y_test = train_test_split(tEmb_train, tYX.Y, train_size=0.8, random_state=22, shuffle = True, stratify = tYX.Y)\n","\n","  m_svc = LinearSVC(random_state=22, max_iter=10000, C=0.007, dual=False, fit_intercept=True).fit(X_train, y_train)  # SVC is ok to use. See updated Rules.\n","  print(\"Train Accuracy: \", m_svc.score(X_train, y_train))   # in-sample accuracy\n","  print(\"Validation Accuracy: \", m_svc.score(X_test, y_test))\n","  print(\"Accuracy: \", m_svc.score(tEmb_train,tYX.Y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1PsK7LZJIgQ"},"outputs":[],"source":["pY_linSVC = pd.DataFrame(m_svc.predict(vEmb_test), index=vX.index, columns=['y'])   # predicted targets\n","ToCSV((pY_linSVC>0.5)*1, 'LinearSVC_with_kmeans_and_0score1score')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1658717843516,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"},"user_tz":240},"id":"ZnfCxUgPvzPh","outputId":"f42647e0-8a18-4a44-ea17-9b3b0b8391f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["y\n","0    10012\n","1     9988\n","dtype: int64"]},"metadata":{},"execution_count":47}],"source":["pY_linSVC.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"BhoznQ8LDaie"},"source":["### Baseline Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9X0uJ5bo499"},"outputs":[],"source":["# tf.random.set_seed(0)   # always seed your experiments\n","# Init = keras.initializers.RandomNormal(seed=0)\n","# import numpy as np\n","\n","# m = keras.models.Sequential([\n","#     Flatten(input_shape=[tEmb.shape[1]]),\n","#     Dense(100, activation=\"relu\", kernel_initializer=Init),\n","#     Dense(1, activation='sigmoid', kernel_initializer=Init)])\n","# m.summary()\n","# m.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=['binary_accuracy'])\n","# hist = m.fit(tEmb, np.array(tYX.Y), batch_size=64, epochs=10, validation_split=0.3)"]},{"cell_type":"markdown","metadata":{"id":"pzBsjCvS_kEw"},"source":["# **References:**"]},{"cell_type":"markdown","metadata":{"id":"2kr8Q-9T_nAb"},"source":["1. https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/\n","2. G√©ron, A. (2019). Hands-on machine learning with scikit-learn, keras, and tensorflow: Concepts, tools, and techniques to build intelligent systems (2nd edition). O'Reilly Media.\n","3. https://medium.com/mlearning-ai/apply-machine-learning-algorithms-for-genomics-data-classification-132972933723#f04f\n","4. https://sourmash.readthedocs.io/en/latest/kmers-and-minhash.html\n","5. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"]},{"cell_type":"markdown","metadata":{"id":"DoF2GoB_QGw9"},"source":["<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n","\n","<hr color=red>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nosV1OWFJPx5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658717822503,"user_tz":240,"elapsed":6,"user":{"displayName":"Delta Solutions","userId":"12152604300267225306"}},"outputId":"ef0c88f0-c87d-45e4-c631-bf01451cb27a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Runtime is 246 sec\n"]}],"source":["tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."]},{"cell_type":"markdown","metadata":{"id":"udpl5HJ4JSLr"},"source":["# üí°**Starter Ideas**"]},{"cell_type":"markdown","metadata":{"id":"uWllDRgiJVDJ"},"source":["1. Learn about DNA [&#127910;](https://www.youtube.com/results?search_query=nucleotides+genes+amino+acids+)\n","1. Try a larger training sample. \n","1. Try longer training DNA strings, but SBERT may have a cap on string length, so you might split DNA into several strings and then concatenate or average resulting vectors\n","1. Try other pretrained SBERT models. Note that DNA sequence uses ACGT letters, but many other models were trained on multilingual text. So, you might prefer those that were trained on mostly ASCII.\n","1. SBERT is trained on word tokens (typically, separated by spaces), but DNA sequence has no spaces. Try placing spaces after every character or some semantically meaningful subsequences (this might require more domain knowledge).\n","1. Try Google's [USE](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) embedding models\n","1. Try Facebook's [LASER](https://github.com/facebookresearch/LASER) and [others](https://tfhub.dev/s?module-type=text-language-model). \n","1. Try [Enformer](https://tfhub.dev/deepmind/enformer/1) for gene expressions. See [DeepMind paper](https://deepmind.com/blog/article/enformer).\n","1. Try building your own embeddings on the given sequences. SBERT and other packages make it easy (just a few lines), but it may take too much time.\n","1. Assess distribution of character patterns (single, doubles, triplets, ...). For example, an ACGT string generates AC, CG, GT doubles and ACG and CGT triplets. Does one class have more subsequences of some type? This might be a feature in your model. \n","1. Try features built as counts of subsequences (singles, doubles, triplets, ...). Consider EDA first.\n","1. Concatenate or otherwise combine multiple embeddings derived from each gene string\n","1. Learn from [*The genetic code*](https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/the-genetic-code-discovery-and-properties), Khan Academy.\n","1. Learn from [*Apply Machine Learning Algorithms for Genomics Data Classification*](https://medium.com/mlearning-ai/apply-machine-learning-algorithms-for-genomics-data-classification-132972933723)\n","1. Learn from [*Efficient counting of k-mers in DNA sequences using a bloom filter*](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-333) P√°ll Melsted et al. 2011\n","1. Try [Byte Pair Encoding](https://www.derczynski.com/papers/archive/BPE_Gage.pdf) and [SentencePiece](https://arxiv.org/pdf/1808.06226.pdf) to auto identification of \"important\" [k-mers](https://en.wikipedia.org/wiki/K-mer) (substrings)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of üêß1-[TheSelfishGene]-Na, Rivas, Yoo.ipynb","provenance":[{"file_id":"1xU62Eri0kR4HAyuODUHZCN1EnKBp_REq","timestamp":1658718694085},{"file_id":"1JMaJkTxxafsY_dep-4vQY-rH7YH1xGDz","timestamp":1658155125359}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}